#!/usr/bin/env python
import sys
import ConfigParser
import shutil
import subprocess
import re
import time
import os
import os.path as p
from optparse import OptionParser
from optparse import OptionGroup


class Pipeline:
    '''
    This class will construct a pipeline object
    based on configuration files.  Use the
    run() function to execute the script.
    
    '''
    
    # Current shortcomings of this class include:
    #    Not validating that the .ncbirc file is setup and that
    #        the shared and local directories exist.
    #    Which method of executing jobs to use
    def __init__(self, cli_options):
        # Initializing the Pipeline class includes getting configuring and 
        # validating the parameters from the configuration file, setting up 
        # all of the class variables and the creating the directory 
        # structure needed to execute everything.
        
        self.__fatal_error = False
        self.params = {}
        
        self.params = self.__configure(cli_options)
        self.__validate()
        
        self.directories = self.__get_directories()
        self.preferences = self.__get_preferences()
        self.static_vars = self.__get_static_vars()
        self.qsub_options = self.__get_qsub_options()
        
        self.type = self.preferences['Execution']
        self.debug = self.params['debug']
        self.verbose = self.params['verbose']
        
                
        self.__setup_directory_structure()
        
        if self.params['debug']:        
            print "Directories:"
            for (k,v) in self.directories.iteritems():
                print "\t"+str(k)+": "+str(v)
            print "Preferences:"
            for (k,v) in self.preferences.iteritems():
                print "\t"+str(k)+": "+str(v)
            print "Static Variables:"
            for (k,v) in self.static_vars.iteritems():
                print "\t"+str(k)+": "+str(v)
            print "qsub Options:"
            for (k,v) in self.qsub_options.iteritems():
                print "\t"+str(k)+": "+str(v)

        
    def run(self):
        # Runs the actual pipeline scripts in the correct order and
        # passes the id given from qsub from one job to the next.
                
        self.__backup_configuration()
        
        id = self.__run_pipeline_prep()
        id = self.__run_fasta_prep(id)
        id = self.__run_get_good_seqs(id)
        id = self.__run_blast_dir(id)
        id = self.__run_blast_arr_prep(id)
        id = self.__run_blast_arr(id)
        id = self.__run_blast_arr_check(id)
        id = self.__run_blastall_hits(id)
        id = self.__run_clustal_prep(id)
        id = self.__run_clustal(id)
        id = self.__run_clustal_check(id)
        id = self.__run_alignment(id)
        id = self.__run_dist_matrix(id)
        id = self.__run_neighbor(id)
        id = self.__run_finalize(id)
        
        # If everything has executed,        
        return 0


    def __run_pipeline_prep(self):
        # No local variables to pass
        variables = {}
        
        cmd_options = {
            'name':             "pipeline_prep",
            'log':              p.join(self.directories['logs'], 
                                             "pipeline_prep.log"),
            'variables':        variables,
            'script_location':  p.join(self.directories['scripts'], 
                                             "pipeline_prep.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_fasta_prep(self, id=None):
        variables = {
            'Input_Sequences_List':         p.join(
                                                self.directories['blast_input'],
                                                "input_sequences_list")
        }
        
        cmd_options = {
            'name':                         "fasta_prep",
            'log':                          p.join(
                                                self.directories['logs'], 
                                                "fasta_prep.log"),
            'variables':                    variables,
            'previous_id':                  id,
            'script_location':              p.join(
                                                self.directories['scripts'], 
                                                "fasta_files_prep.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_get_good_seqs(self, id=None):
        variables = {
            'Input_Sequences_File':        p.join(
                                               self.directories['blast_input'],
                                               "input_sequences")
        }
        
        cmd_options = {
            'name':                         "get_good_seqs",
            'log':                          p.join(
                                                self.directories['logs'], 
                                                "get_good_sequences.log"),
            'variables':                    variables,
            'previous_id':                  id,
            'script_location':              p.join(
                                                self.directories['scripts'], 
                                                "get_good_sequences.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_blast_dir(self, id=None):
        variables = {
            'Direction_Blast_File':         p.join(
                                                self.directories['blast'],
                                                "direction_blast")
        }
        
        cmd_options = {
            'name':                         "blast_direction",
            'log':                          p.join(
                                                self.directories['logs'], 
                                                "blast_direction.log"),
            'variables':                    variables,
            'previous_id':                  id,
            'parallel':                     True,
            'script_location':              p.join(
                                                self.directories['scripts'], 
                                                "blast_direction.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_blast_arr_prep(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "blast_arr_prep",
            'log':                  p.join(self.directories['logs'], 
                                                "blastall_array_prep.log"),
            'variables':            variables,
            'parallel':             True,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "blastall_array_prep.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_blast_arr(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "blast_arr",
            'log':                  p.join(self.directories['logs'], 
                                                "blastall_array.log"),
            'variables':            variables,
            'array':                True,
            'parallel':             True,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "blastall_array.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_blast_arr_check(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "blast_arr_check",
            'log':                  p.join(self.directories['logs'], 
                                                "blastall_array_check.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "blastall_array_check.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_blastall_hits(self, id=None):
        variables = {
            'Blast_Out_5_File':     p.join(self.directories['blast'],
                                                 "blastout5"),
            'Hit_Seqs_File':        p.join(self.directories['clustal'],
                                                 "hitseqs")                     
        }
        cmd_options = {
            'name':                 "blast_hits",
            'log':                  p.join(self.directories['logs'], 
                                                "blastall_hits.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "blastall_hits.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_clustal_prep(self, id=None):
        variables = {
            'Clustal_File':         p.join(self.directories['clustal'],
                                                 "clustal")                    
        }
        cmd_options = {
            'name':                 "clustal_prep",
            'log':                  p.join(self.directories['logs'], 
                                                "clustal_prep.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "clustal_prep.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_clustal(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "clustal_run",
            'log':                  p.join(self.directories['logs'], 
                                                "clustal_run.log"),
            'variables':            variables,
            'parallel':             True,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "clustal_run.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_clustal_check(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "clustal_check",
            'log':                  p.join(self.directories['logs'], 
                                                "clustal_check.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "clustal_check.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_alignment(self, id=None):
        variables = {}
        cmd_options = {
            'name':                 "alignment",
            'log':                  p.join(self.directories['logs'], 
                                                "alignment.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "alignment.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_dist_matrix(self, id=None):
        variables = {
            'DNADist_Script':       p.join(self.directories['scripts'],
                                                 "dnadist_script"),
            'Distances_File':       p.join(self.directories['clustal'],
                                                 "distances")
        }
        cmd_options = {
            'name':                 "dist_matrix",
            'log':                  p.join(self.directories['logs'], 
                                                "distance_matrix.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "distance_matrix.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_neighbor(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "neighbor",
            'log':                  p.join(self.directories['logs'], 
                                                "neighbor_run.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "neighbor_run.sh")
        }
        
        return self.execute_job(cmd_options)
    def __run_finalize(self, id=None):
        variables = {
            'Final_Log':            p.join(self.directories['output'],
                                                 "pipeline.log")
        }
        cmd_options = {
            'name':                 "finalize",
            'log':                  p.join(self.directories['logs'], 
                                                "finalize.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      p.join(self.directories['scripts'], 
                                                "final_cleanup.sh")
        }
        
        return self.execute_job(cmd_options)


    def execute_job(self, options):
        # Calls subprocess.Popen to execute the command.
        # If in standalone mode, it directly outputs stdout
        # to the screen.  If running in parallel mode, it suppresses
        # stdout and instead prints its own messages.  The script
        # output will be sent to a log file by qsub when it executes
        # the script.
        
        # This currently points to one of two functions, either
        # execute_job_split() or execute_job_nonsplit().
        # These are poor names and are just placeholders while deciding
        # which function to actually use, which will then get cut and pasted
        # into this section.
        
        id = self.execute_job_nonsplit(options)
        return id
    def __execute_job_array_prep(self, job_name, hold_filepath, error_filepath):
        arr_count = -1
        while p.exists(hold_filepath) is not True:
            time.sleep(10)
            if p.exists(error_filepath):
                os.remove(error_filepath)
                self.__set_fatal_error(True, 
                                       err_msg="a job prior to the array "
                                       "failed and the pipeline "
                                       "cannot recover")
                
        file = open(hold_filepath, "r")
        arr_count = file.readline()
        arr_count = str(arr_count).strip()
    
        os.remove(hold_filepath)
        
        if self.params['debug']:
            print "Number of array items: "+str(int(arr_count))
    
        return int(arr_count)  
    def __execute_job_array_check(self, job_name, array_count, base_id):
        # This is not the best way to check the status of the job array
        # because if any of the jobs leave qstat's scope before it is
        # checked, this will cause a world of hurt.
        # This pipes all output to devnull so that we don't end up in
        # deadlock from trying to read or write to a blocked pipe.
        # May want to look at changing subprocess.Popen() to subprocess.call()
        # since we're using /dev/null.
        if self.type == "Parallel":
            array_count += 1 # xrange is exclusive
            devnull = open('/dev/null', 'w')
            completed_list = []
            for i in xrange(array_count):
                completed_list.append(i)
            
            if self.params['debug']:
                print "Number of jobs in list: "+str(array_count)
                print "Printing completed_list[]:"
                print "\t"+str(completed_list)+"\n"
            if self.verbose or self.debug:
                msg = "Waiting for \""+job_name+"\" to finish..." 
                print msg
            while completed_list:
                for (index,value) in enumerate(completed_list):
                    cmd = "qstat -f " + str(base_id) \
                            + "[" + str(value) + "] | grep \"job_state = C\""
                    process = subprocess.Popen(cmd, shell=True, 
                                               stdout=devnull, stderr=devnull)
                    out, err = process.communicate()
                    if process.returncode == 0:
                        if self.debug:
                            msg = "array["+str(value)+"] completed, " \
                                    "removing from list."
                            print msg
                        completed_list.remove(value)
                    else:
                        time.sleep(10)
                        break
            devnull.close()
        return True
    def __get_command(self, cmd_options):
         if self.type != "Parallel":
             # Execute in standalone.
             # By executing the command in the form of:
             # `var1=foo var2=bar /path/to/script`
             # var1 and var2 are passed to the script as variables
             # just like qsub would pass them.
             
             var_list = ""
             for (key,value) in self.preferences.iteritems():
                 var_list += str(key).upper()+"=\""+str(value)+"\" "
             for (key,value) in self.static_vars.iteritems():
                 var_list += str(key).upper()+"=\""+str(value)+"\" "
             for (key,value) in cmd_options['variables'].iteritems():
                 var_list += str(key).upper()+"=\""+str(value)+"\" "
             
             # Because these scripts are made to be called by qsub,
             # we need to fake some of the variables that get passed
             # in from qsub.  Currently the only variables used are
             # PBS_O_WORKDIR and PBS_ARRAYID.
             var_list += "PBS_O_WORKDIR=\""
             var_list += str(self.directories['output'])+"\" "
             
             if cmd_options.has_key('array_id'):
                 var_list += "PBS_ARRAYID=\""
                 var_list += str(cmd_options['array_id'])+"\" "    
             
             var_list = var_list.rstrip()
             cmd = var_list + " " + str(cmd_options['script_location'])
         else:
             # Execute in parallel
             # Add all of the options that qsub needs.
             cmd = "qsub "
             cmd += "-N "+str(cmd_options['name'])+" "
             cmd += "-j oe "
             cmd += "-o "+str(cmd_options['log'])+" "
             cmd += "-d "+str(self.directories['output'])+" "
             cmd += "-q "+str(self.qsub_options['Queue'])+" "
             
             if cmd_options.has_key('parallel'):
                 cmd += "-l nodes="+str(self.qsub_options['Nodes'])+" "
             if cmd_options.has_key('arr_count'):
                 cmd_options['arr_count'] -= 1
                 cmd += "-t 0-"+str(cmd_options['arr_count'])+" "
             if cmd_options.has_key('previous_id'):
                 if cmd_options['previous_id'] is not None:
                     cmd += "-W depend=afterok:"
                     cmd += cmd_options['previous_id']+" "
             var_list = ""
             for (key,value) in self.preferences.iteritems():
                 var_list += str(key).upper()+"="+str(value)+","
             for (key,value) in self.static_vars.iteritems():
                 var_list += str(key).upper()+"="+str(value)+","
             for (key,value) in cmd_options['variables'].iteritems():
                 var_list += str(key).upper()+"="+str(value)+","
             
             # Give the script the number of nodes to use.  This should
             # eventually become a static variable.
             if cmd_options.has_key('parallel'):
                 var_list += "NNODES="+str(self.qsub_options['Nodes'])+","
             
             var_list = var_list.rstrip(", ")
             if var_list is not None:
                 cmd += "-v "+var_list+" "
                 
             # Add the actual script to execute to the end.
             cmd += cmd_options['script_location']
         return cmd 
    def execute_job_split(self, options):
        # Gets the command and executes based on if in parallel or standalone
        # modes.  The parallel and standalone portions are completely
        # separated from each other, so if code needs to be modified, it will
        # more than likely need to be modified in two locations.
        # The up side, is that the code is a bit easier to read.
        
        id = None
        if self.type == "Parallel":
            
            if options.has_key('array'):
                if self.params['verbose'] or self.params['debug']:
                    msg = "Job \""+str(options['name'])+"\" is an array job "\
                            "and will not be executed until the previous job "\
                            "is complete."
                    print msg
                options['arr_count'] = self.__execute_job_array_prep(
                                       options['name'], 
                                       self.static_vars['Array_Output_File'], 
                                       self.static_vars['Error_File'])
                
                # We know the job is complete and so we don't want to have
                # a job dependency.
                options['previous_id'] = None
                if self.params['verbose'] or self.params['debug']:
                    msg = "Number of array items in job: "\
                            +str(options['arr_count'])
                    print msg
                    
            cmd = self.__get_command(options)
            if self.params['debug']:
                print "===================Executing==================="
                print "\t"+str(cmd)
                print "==============================================="
                
            process = subprocess.Popen(cmd, shell=True,
                                        stdout=subprocess.PIPE, 
                                        stderr=subprocess.STDOUT)
            out, err = process.communicate()
                    
            id = re.split('[[\.]{1}',out)[0]
            if self.params['verbose'] or self.params['debug']:
                print options['name'] + " submitted. ID: "+str(id)
            if options.has_key('arr_count') and options['arr_count'] >= 0:
                if self.verbose or self.debug:
                    msg = "Waiting for the array job \""\
                            +str(options['name'])\
                            +"\" to finish before continuing ("\
                            +str(options['arr_count'])+" jobs in array)."
                    print msg
                
                self.__execute_job_array_check(options['name'], 
                                             options['arr_count'], 
                                             id)
                id = None # We don't want the next job to have a dependency
                    
        else: # Standalone
            loop_count = 1
            if options.has_key('array'):
                loop_count = self.__execute_job_array_prep(
                                       options['name'], 
                                       self.static_vars['Array_Output_File'], 
                                       self.static_vars['Error_File'])
            for i in xrange(loop_count):
                options['array_id'] = i
                cmd = self.__get_command(options)
                if self.params['debug']:
                    print "===================Executing==================="
                    print "\t"+str(cmd)
                    print "==============================================="
                
                # Doesn't log anything.    
                #return_code = subprocess.call(cmd, shell=True)
                #if return_code != 0:
                #    msg = "The job "+str(options['name'])+" failed"
                #    self.__set_fatal_error(True, msg)
                        
                process = subprocess.Popen(cmd, shell=True,
                                          stdout=subprocess.PIPE, 
                                          stderr=subprocess.STDOUT)
                out, err = process.communicate()
                if out is not None:
                    if self.verbose or self.debug:
                        print str(out) # stderr is redirected to stdout
                    
                    logname = str(options['log'])
                    
                    # Mimic qsub's log naming convention
                    if options.has_key('array'):
                        logname += "-" + str(options['array_id'])
                        
                    # Write the log file
                    fp = open(logname,"w")
                    fp.write(out)
                    fp.close()
                if process.returncode != 0:
                    msg = "The job "+str(options['name'])+" failed"
                    self.__set_fatal_error(True, msg)
        return id
    def execute_job_nonsplit(self, options):
        # Gets the command and executes based on if in parallel or standalone
        # modes.  The parallel and standalone portions are mixed in with 
        # each other, so if code needs to be modified, it will only need to
        # be modified in one location.  This however, makes the code harder
        # to read.
        # The up side, is that the code isn't repeated, which can cut down
        # on errors during maintenance.
        
        id = None
        loop_count = 1
        
        if options.has_key('array'):
            if (self.type == "Parallel") and (self.verbose or self.debug):
                msg = "Job \""+str(options['name'])+"\" is an array job "\
                        "and will not be executed until the previous job "\
                        "is complete."
                print msg
            options['arr_count'] = self.__execute_job_array_prep(
                       options['name'], 
                       self.static_vars['Array_Output_File'], 
                       self.static_vars['Error_File'])
            if self.type == "Parallel":
                options['previous_id'] = None
                if self.verbose or self.debug:
                    msg = "Number of array items in job: "\
                            +str(options['arr_count'])
                    print msg
            else:
                loop_count = options['arr_count']
        for i in xrange(loop_count):
            if self.type != "Parallel":
                options['array_id'] = i
            cmd = self.__get_command(options)
            if self.debug:
                print "===================Executing==================="
                print "\t"+str(cmd)
                print "==============================================="                        
            process = subprocess.Popen(cmd, shell=True,
                                      stdout=subprocess.PIPE, 
                                      stderr=subprocess.STDOUT)
            out, err = process.communicate()
                
            if self.type == "Parallel":
                id = re.split('[[\.]{1}',out)[0]
                if self.verbose or self.debug:
                    print options['name'] + " submitted. ID: "+str(id)
                    if options.has_key('arr_count'):
                        msg = "Waiting for the array job \""\
                                +str(options['name'])+"\" to finish before "\
                                "continuing ("+str(options['arr_count'])\
                                +" jobs in array)."
                        print msg
                        self.__execute_job_array_check(options['name'], 
                                                 options['arr_count'], 
                                                 id)
                        id = None # Next job has no dependency
            else: # Standalone
                if out is not None:
                    if self.verbose or self.debug:
                        print str(out) # stderr is redirected to stdout
                    
                    logname = str(options['log'])
                    
                    # Mimic qsub's log name
                    if options.has_key('array'):
                        logname += "-" + str(options['array_id'])
                    fp = open(logname,"w")
                    fp.write(out)
                    fp.close()
            if process.returncode != 0:
                if self.type == "Parallel":
                    msg = "qsub failed to submit: "+str(options['name'])
                else:
                    msg = "The job "+str(options['name'])+" failed"
                self.__set_fatal_error(True, msg)
        return id


    def __configure(self, cli_options):
        # Given the location of the configuration file(s) and the
        # options passed in from the command line, set up a
        # a dictionary, 'p', with the parameters that will be used.
        # This dictionary will be used to set up all of our
        # variables that are used throughout the pipeline.
        # The ConfigParser object does have a way to do string substitution,
        # and by moving to that style we could eliminate almost all of the
        # command line options, leaving us with just the verbose/debug options
        # and one configuration file.
         
        options = {
            'scripts_dir':          cli_options.scripts_dir,
            'work_dir':             cli_options.work_dir,
            'debug':                cli_options.debug,
            'verbose':              cli_options.verbose
        }
        
        default_qsub_options = {
            'Email':                "None",
            'NotifyOnAbort':        "True",
            'NotifyOnBegin':        'True',
            'NotifyOnEnd':          'True',
            'Queue':                'tiny',
            'Nodes':                '50'
        }
        
        default_sequencing_options = {
            'Execution':            'Standalone',
            'Database':             '/mnt/home/cblair/rdp/species/species',
            'InputSequences':       '$work_dir/input/sequences',
            'ReferenceStrains':     '$work_dir/input/RefStrains',
            'BlastSequences':       '$work_dir/input/af243169.for',
            'Suffix':               '.fasta',
            'Direction':            'Forward',
            'CutoffLength':         '50',
            'MaxBlasts':            '50',
            'MinSequenceLength':    '300',
            'NHits':                '25',
            'NPercent':             '.01',
            'Root':                 'Methanococcus_jannaschii',
            'Primer3':              'GACTCGGTCC',
            'Primer5':              'CCTAGTGGAGG'
        }
        
        # Default options
        options = dict(options.items() + 
                       default_qsub_options.items() + 
                       default_sequencing_options.items())
        parser = ConfigParser.ConfigParser()
        parser.optionxform = str # Forces case sensitivity
        parser.read(cli_options.config_file)
        
        # Parse the sections and add to parameters
        for sections in parser.sections():
            items = parser.items(sections)
            for (key, value) in items:
                options[key] = value  
                      
        # Perform keyword substitution
        for (key, value) in options.iteritems():        
            if str(value).find("$scripts_dir") != -1:
                value = p.abspath(
                        value.replace("$scripts_dir", options['scripts_dir'])) 
            if str(value).find("$work_dir") != -1:
                value = p.abspath(
                        value.replace("$work_dir", options['work_dir']))
            if (str(value)).find("None") != -1:
                value = None
            if (str(value)).find("True") != -1:
                value = True
            elif (str(value)).find("False") != -1:
                value = False
            options[key] = value
        
        
        # Omitting this for now.  In a future version this portion will
        # check to see if the environment variables that mpiBLAST needs
        # are set and if not, then check to see if the .ncbirc file exists
        # and extract the information from there.  If the 'MPIBlastDB' key
        # doesn't exist, then __validate() will flag it and fail (if 
        # running in parallel).
        # Check to make sure that the .ncbirc file exists
        #if options['Execution'] == "Parallel":    
        #    ncbi_parser = ConfigParser.ConfigParser()
        #    ncbi_parser.optionxform = str
        #    ncbi_parser.read(p.abspath(p.expanduser("~/.ncbirc")))
        #    if ncbi_parser.has_section("mpiBLAST"):
        #        if ncbi_parser.has_option("mpiBLAST", "Shared"):
        #            options['MPIBlastDB'] = ncbi_parser.get("mpiBLAST",
        #                                                    "Shared")
                    
                
        return options
    def __validate(self):
        # Because the configuration parser has default values built in, the
        # only validation that's currently necessary is that the scripts
        # directory and work directory exist.
        # This should check to make sure the .ncbirc file exists and that
        # the local and shared directories under mpiBLAST exist.

        is_valid = p.exists(self.params['scripts_dir'])
        self.__set_fatal_error(is_fatal=not is_valid,
                               err_msg="The scripts directory doesn't exist.")
        
        is_valid = p.exists(self.params['work_dir'])
        self.__set_fatal_error(is_fatal=not is_valid,
                               err_msg="The working directory doesn't exist.")
        
        # Omitting this for now.  In a future version this portion will
        # check to see if the 'MPIBlastDB' variable has been set and error
        # if not (if running in parallel).
        #if self.params['Execution'] == "Parallel":
        #    is_valid = self.params.has_key('MPIBlastDB')
        #    self.__set_fatal_error(is_fatal=not is_valid,
        #                       err_msg="'Shared' variable in .ncbirc file "\
        #                       "is not set")
        
        
    def __get_directories(self):
        # Return a dictionary of the directories, based on the parameters
        # that were configured.
        
        d = {
             'work':                    self.params['work_dir'],
             'scripts':                 self.params['scripts_dir']
        }
        d['output']         =           p.join(d['work'],'output')
        d['logs']           =           p.join(d['output'],'logs')
        d['references']     =           p.join(d['output'],'references')
        d['blast']          =           p.join(d['output'],'blast')
        d['clustal']        =           p.join(d['output'],'clustal')
        d['tree']           =           p.join(d['output'],'tree')
        
        d['originals']      =           p.join(d['references'],
                                                     'originals')
        
        d['blast_input']    =           p.join(d['blast'],'input')
        d['blast_temp']     =           p.join(d['blast'],'tmp')
        d['blast_output']   =           p.join(d['blast'],'blasts')
        
        # Omitting this for now.  Uncomment once 'MPIBlastDB' portion
        # is implemented.  For now, we are assuming that the user
        # has their environment variable/.ncbirc file set up correctly.
        #if self.params['Execution'] == "Parallel":
        #    d['blast_db']   =           self.params['MPIBlastDB'] 
        
        return d
    def __get_preferences(self):
        # Return a dictionary of the preferences variables, based on the
        # parameters that were configured.
        # If qsub isn't found on the system and the execution was
        # set to be parallel, then execution is set
        # to be forced into standalone.
        # The 'Debug' preference currently is a command line argument and 
        # if the debug flag is true then both this pipeline class and the 
        # scripts it executes be set by the same flag.  The script's debug
        # option could instead come from the configuration file.
         
        p = {
             'Execution':               self.params['Execution'],
             'Database':                self.params['Database'],
             'Input_Sequences':         self.params['InputSequences'],
             'Reference_Strains':       self.params['ReferenceStrains'],
             'Blast_Sequences':         self.params['BlastSequences'],
             'Suffix':                  self.params['Suffix'],
             'Direction':               self.params['Direction'],
             'Cutoff_Length':           self.params['CutoffLength'],
             'Max_Blasts':              self.params['MaxBlasts'],
             'Min_Sequence_Length':     self.params['MinSequenceLength'],
             'NHits':                   self.params['NHits'],
             'NPercent':                self.params['NPercent'],
             'Root':                    self.params['Root'],
             'Primer3':                 self.params['Primer3'],
             'Primer5':                 self.params['Primer5'],
             'Debug':                   self.params['debug']                          
        }
        # Make sure qsub exists on the system
        if p['Execution'] == 'Parallel':
            does_qsub_exist = subprocess.Popen(["which","qsub"],
                                               stdout=subprocess.PIPE,
                                               stdin=subprocess.PIPE,
                                               stderr=subprocess.PIPE,
                                               shell=False)
            does_qsub_exist.communicate() # Don't care about output.
            if does_qsub_exist.returncode != 0:
                p['Execution'] = "Forced Standalone"
                
        return p
    def __get_static_vars(self):
        # Returns a dictionary of variables that are used between the scripts
        # to make sure that anything that uses the same variable is
        # always pointing to the same value.
        
        s = {
            'Perl_Dir':                 self.directories['scripts'],
            'References_Dir':           self.directories['references'],
            'Originals_Dir':            self.directories['originals'],
            'Log_Dir':                  self.directories['logs'],
            'Blast_Dir':                self.directories['blast'],
            'Blast_Temp_Dir':           self.directories['blast_temp'],
            'Blastall_Output_Dir':      self.directories['blast_output'],
            'MPIBlast_Shared':          self.directories['blast_db'],
            'Hit_Output_Dir':           self.directories['clustal'],
            'Clustal_Output_Dir':       self.directories['clustal'],
            'Neighbor_Dir':             self.directories['clustal'],
            'Tree_Dir':                 self.directories['tree']
        }
        s['Good_Sequences_File'] = p.join(self.directories['blast_input'],
                                                "good_sequences")
        s['Numseqs_Temp_File'] = p.join(s['Blast_Temp_Dir'],
                                              "numseqs.tmp")
        s['Blast_Input_File'] = p.join(self.directories['blast_input'],
                                             "blast_input")
        s['Hit_File'] = p.join(s['Clustal_Output_Dir'], "hitfiles")
        s['Clustal_All_File'] = p.join(s['Clustal_Output_Dir'],
                                             "clustal_all")
        s['Clustal_Alignment_File'] = p.join(s['Clustal_Output_Dir'],
                                                   "clustal_all.aln")
        s['Phylip_In_File'] = p.join(s['Clustal_Output_Dir'],
                                            "infile")
        s['Array_Output_File'] = p.join(self.directories['output'],
                                              "arrayjob.tmp")
        s['Error_File'] = p.join(self.directories['output'], "error.tmp")
        return s
    def __get_qsub_options(self):
        # Return a dictionary of the options to be passed to qsub, based
        # on the parameters that were configured.
        
        q = {
             'Email':                   self.params['Email'],
             'NotifyOnAbort':           self.params['NotifyOnAbort'],
             'NotifyOnBegin':           self.params['NotifyOnBegin'],
             'NotifyOnEnd':             self.params['NotifyOnEnd'],
             'Queue':                   self.params['Queue'],
             'Nodes':                   self.params['Nodes']
        }
        return q
    def __setup_directory_structure(self):
        # This method removes the 'output' directory if it exists
        # and then recreates the directory and any subdirectories
        # that it needs.  Because the directories dictionary isn't
        # an ordered dictionary (not avail in Python 2.6.6), we can't
        # loop through the dictionary easily to create the subdirectories.
        
        # Remove the output directory if it exists and recreate it
        if p.exists(self.directories['output']):
            shutil.rmtree(self.directories['output'])
        os.mkdir(self.directories['output'])
        
        # Subdirectories of 'output'
        os.mkdir(self.directories['logs'])
        os.mkdir(self.directories['references'])
        os.mkdir(self.directories['blast'])
        os.mkdir(self.directories['clustal'])
        os.mkdir(self.directories['tree'])
        
        # Subdirectories of 'blast'
        os.mkdir(self.directories['blast_input'])
        os.mkdir(self.directories['blast_temp'])
        os.mkdir(self.directories['blast_output'])
        
        # Make sure that the folder specified in the .ncbirc
        # exists and create it otherwise.
        if self.type == "Parallel":
            if not os.path.exists(self.directories['blast_db']):
                os.mkdir(self.directories['blast_db'])
    def __backup_configuration(self):
        backup_list = {
           'qsub Options':              self.qsub_options,
           'Pipeline Preferences':      self.preferences
       }
        parser = ConfigParser.ConfigParser()
        parser.optionxform = str
        
        for (section,var) in backup_list.iteritems():
            parser.add_section(section)
            for (option, value) in var.iteritems():
                parser.set(section, option, value)
        
        filename = p.join(self.directories['references'],
                                'used_preferences.conf')
        fp = open(filename,"w")
        parser.write(fp)
        fp.close() 
    def __check_for_fatal(self):
        return self.__fatal_error
    def __set_fatal_error(self, is_fatal, err_msg=""):
        if not self.__check_for_fatal():
            self.__fatal_error = is_fatal
        if self.__check_for_fatal():
            print >> sys.stderr, "ERROR: "+err_msg+" Exiting."
            sys.exit(1)

            
def main():
    # Set up an OptionParser object to make the command line arguments easy.
    # NOTE:  optparse has been deprecated since Python 2.7, but as of May 2013,
    #        the cluster is using Python 2.4.3. argparse should be used once
    #        a newer version of Python is in use.
    
    parser = OptionParser(usage="%prog [options] \n"
                          "HiTSA Pipeline for Beowulf cluster using TORQUE\n"
                          "Results may differ between standalone and parallel "
                          "executions due to different versions of blastall "
                          "and clustalw being used.")
    parser.add_option("-c", "--config", action="store", dest="config_file",
                      help="the file where the your preferences are "\
                            "located[default: %default]")
    parser.add_option("-w", "--work", action="store", dest="work_dir",
                      help="the working directory where all output will be "\
                            "placed [default: %default]")
    parser.add_option("-v", "--verbose", action="store_true", dest="verbose",
                      help="show basic output [default: %default]")
    parser.add_option("-q", "--quiet", action="store_false", dest="verbose",
                      help="hide basic output [default: %default]")
    
    adv_group = OptionGroup(parser, "Advanced Options", 
                            "Use these options at your own risk. "\
                            "Changing these options could cause "\
                            "the program to fail.")
    adv_group.add_option("-s", "--scripts", action="store", dest="scripts_dir",
                         help="the directory where the script directories "
                                "are located [default: %default]")
    parser.add_option_group(adv_group)
    debug_group = OptionGroup(parser, "Debug Options",
                              "Use this to get additional output about the "\
                              "variables used in each of the modules.")
    debug_group.add_option("-d", "--debug", action="store_true", dest="debug",
                           help="will display additional output for "\
                           "debugging purposes both in the pipeline and "\
                           "individual jobs as well [default: %default]")
    parser.add_option_group(debug_group)
    
    # Set up the defaults
    parser.set_default("scripts_dir", p.abspath(p.join(
                               p.dirname(p.expanduser(__file__)),"scripts")))
    parser.set_default("work_dir", p.abspath(p.join(p.dirname(__file__),".")))
    parser.set_default("verbose", True)
    parser.set_default("config_file", p.abspath(p.join(
                               p.dirname(p.expanduser(__file__)),
                                            "./preferences.conf")))
    parser.set_default("debug", False)
    
    # Parse the arguments and return a tuple of the options and the
    # any leftover arguments.  If there are any leftover arguments, we
    # will throw an error.
    (options, remaining_arguments) = parser.parse_args()
    
    if len(remaining_arguments) > 0:
        # An incorrect number of arguments was given.
        # Pass an error message to the parser's error function that will
        # display the error, proper usage and then exit.
        error_message = "There were an incorrect number of arguments passed.\n"
        error_message += "The argument(s) that couldn't be parsed were:\n"
        for arg in remaining_arguments:
            error_message += arg+"\n"
        error_message += "Please see the usage above and try again."
        parser.error(error_message)
        
    # Handle user input    
    options.scripts_dir = p.abspath(p.expanduser(options.scripts_dir))
    options.work_dir = p.abspath(p.expanduser(options.work_dir))
    options.config_file = p.abspath(p.expanduser(options.config_file))
    
    pipeline = Pipeline(options)
    sys.exit(pipeline.run())

    
if __name__ == '__main__':
    main()