#!/usr/bin/env python

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.

import sys
import ConfigParser
import shutil
import subprocess
import re
import time
import os
import os.path as p
from optparse import OptionParser
from optparse import OptionGroup

def expan(filepath=None, joinpath=None):
    # Returns the absolute path of filepath.  If joinpath 
    # was specified, join the files as well.  If filepath
    # is None, just return None.  This lets us not have
    # to check for None everywhere in the Pipeline
    
    f = filepath
    j = joinpath
    
    if f is not None: 
        f = p.abspath(p.expanduser(f))
        if j is not None:
            f = p.abspath(p.join(f,j))
    return f

class Pipeline:
    '''
    This class will construct a pipeline object
    based on configuration files.  Use the
    run() function to execute the script.
    
    '''
    
    def __init__(self, cli_options):
        # Initializing the Pipeline class includes getting configuring and 
        # validating the parameters from the configuration file, setting up 
        # all of the class variables and the creating the directory 
        # structure needed to execute everything.
        
        self.__fatal_error = False
        self.params = {}
        self.params = self.__configure(cli_options)
        
        self.verbose = self.params['Verbose']
        self.quiet = self.params['Quiet']
        
        self.__validate()
        
        self.directories = self.__get_directories()
        self.preferences = self.__get_preferences()
        self.static_vars = self.__get_static_vars()
        self.qsub_options = self.__get_qsub_options()
        
        if self.verbose:        
            print "Directories:"
            for (k,v) in self.directories.iteritems():
                print "\t"+str(k)+": "+str(v)
            print "Preferences:"
            for (k,v) in self.preferences.iteritems():
                print "\t"+str(k)+": "+str(v)
            print "Static Variables:"
            for (k,v) in self.static_vars.iteritems():
                print "\t"+str(k)+": "+str(v)
            print "qsub Options:"
            for (k,v) in self.qsub_options.iteritems():
                print "\t"+str(k)+": "+str(v)      
        
        self.type = self.preferences['Execution']
                
        self.__setup_directory_structure()
        
        
    def run(self):
        # Runs the actual pipeline scripts in the correct order and
        # passes the id given from qsub from one job to the next.
                
        self.__backup_configuration()
        
        id = self.__run_pipeline_prep()
        id = self.__run_fasta_prep(id)
        id = self.__run_get_good_seqs(id)
        id = self.__run_blast_dir(id)
        id = self.__run_blast_arr_prep(id)
        id = self.__run_blast_arr(id)
        id = self.__run_blast_arr_check(id)
        id = self.__run_blastall_hits(id)
        id = self.__run_clustal_prep(id)
        id = self.__run_clustal(id)
        id = self.__run_clustal_check(id)
        id = self.__run_alignment(id)
        id = self.__run_dist_matrix(id)
        id = self.__run_neighbor(id)
        id = self.__run_finalize(id)
        
        # If everything has executed,        
        return 0


    def __run_pipeline_prep(self):
        # No local variables to pass
        variables = {}
        
        cmd_options = {
            'name':             "pipeline_prep",
            'log':              expan(self.directories['logs'], 
                                             "pipeline_prep.log"),
            'variables':        variables,
            'first_script':     True,
            'script_location':  expan(self.directories['scripts'], 
                                             "pipeline_prep.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_fasta_prep(self, id=None):
        variables = {
            'Input_Sequences_List':         expan(
                                                self.directories['blast_input'],
                                                "input_sequences_list")
        }
        
        cmd_options = {
            'name':                         "fasta_prep",
            'log':                          expan(
                                                self.directories['logs'], 
                                                "fasta_prep.log"),
            'variables':                    variables,
            'previous_id':                  id,
            'script_location':              expan(
                                                self.directories['scripts'], 
                                                "fasta_files_prep.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_get_good_seqs(self, id=None):
        variables = {
            'Input_Sequences_File':        expan(
                                               self.directories['blast_input'],
                                               "input_sequences")
        }
        
        cmd_options = {
            'name':                         "get_good_seqs",
            'log':                          expan(
                                                self.directories['logs'], 
                                                "get_good_sequences.log"),
            'variables':                    variables,
            'previous_id':                  id,
            'script_location':              expan(
                                                self.directories['scripts'], 
                                                "get_good_sequences.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_blast_dir(self, id=None):
        variables = {
            'Direction_Blast_File':         expan(
                                                self.directories['blast'],
                                                "direction_blast")
        }
        
        cmd_options = {
            'name':                         "blast_direction",
            'log':                          expan(
                                                self.directories['logs'], 
                                                "blast_direction.log"),
            'variables':                    variables,
            'previous_id':                  id,
            'parallel':                     True,
            'script_location':              expan(
                                                self.directories['scripts'], 
                                                "blast_direction.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_blast_arr_prep(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "blast_arr_prep",
            'log':                  expan(self.directories['logs'], 
                                                "blastall_array_prep.log"),
            'variables':            variables,
            'parallel':             True,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "blastall_array_prep.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_blast_arr(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "blast_arr",
            'log':                  expan(self.directories['logs'], 
                                                "blastall_array.log"),
            'variables':            variables,
            'array':                True,
            'parallel':             True,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "blastall_array.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_blast_arr_check(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "blast_arr_check",
            'log':                  expan(self.directories['logs'], 
                                                "blastall_array_check.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "blastall_array_check.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_blastall_hits(self, id=None):
        variables = {
            'Blast_Out_5_File':     expan(self.directories['blast'],
                                                 "blastout5"),
            'Hit_Seqs_File':        expan(self.directories['clustal'],
                                                 "hitseqs")                   
        }
        cmd_options = {
            'name':                 "blast_hits",
            'log':                  expan(self.directories['logs'], 
                                                "blastall_hits.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "blastall_hits.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_clustal_prep(self, id=None):
        variables = {
            'Clustal_File':         expan(self.directories['clustal'],
                                                 "clustal")                    
        }
        cmd_options = {
            'name':                 "clustal_prep",
            'log':                  expan(self.directories['logs'], 
                                                "clustal_prep.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "clustal_prep.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_clustal(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "clustal_run",
            'log':                  expan(self.directories['logs'], 
                                                "clustal_run.log"),
            'variables':            variables,
            'parallel':             True,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "clustal_run.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_clustal_check(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "clustal_check",
            'log':                  expan(self.directories['logs'], 
                                                "clustal_check.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "clustal_check.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_alignment(self, id=None):
        variables = {}
        cmd_options = {
            'name':                 "alignment",
            'log':                  expan(self.directories['logs'], 
                                                "alignment.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "alignment.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_dist_matrix(self, id=None):
        variables = {
            'DNADist_Script':       expan(self.directories['scripts'],
                                                 "dnadist_script"),
            'Distances_File':       expan(self.directories['clustal'],
                                                 "distances")
        }
        cmd_options = {
            'name':                 "dist_matrix",
            'log':                  expan(self.directories['logs'], 
                                                "distance_matrix.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "distance_matrix.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_neighbor(self, id=None):
        # No local variables to pass
        variables = {}
        cmd_options = {
            'name':                 "neighbor",
            'log':                  expan(self.directories['logs'], 
                                                "neighbor_run.log"),
            'variables':            variables,
            'previous_id':          id,
            'script_location':      expan(self.directories['scripts'], 
                                                "neighbor_run.sh")
        }
        
        return self.__execute_job(cmd_options)
    def __run_finalize(self, id=None):
        variables = {
            'Final_Log':            expan(self.directories['output'],
                                                 "pipeline.log")
        }
        cmd_options = {
            'name':                 "finalize",
            'log':                  expan(self.directories['logs'], 
                                                "finalize.log"),
            'variables':            variables,
            'previous_id':          id,
            'final_script':         True,
            'script_location':      expan(self.directories['scripts'], 
                                                "final_cleanup.sh")
        }
        
        return self.__execute_job(cmd_options)


    def __execute_job(self, options):
        # Gets the command and executes based on if in parallel or standalone
        # modes.  The parallel and standalone portions are completely
        # separated from each other, so if code needs to be modified, it will
        # more than likely need to be modified in two locations.
        # The up side, is that the code is a bit easier to read.
        
        id = None
        to_return = {}
        if self.type == "Parallel":
            if options.has_key('array'):
                if not self.quiet or self.verbose:
                    msg = "Job \""+str(options['name'])+"\" is an array job "\
                            "and will not be executed until the previous job "\
                            "is complete."
                    print msg
                options['arr_count'] = self.__execute_job_array_prep(
                                       options['name'], 
                                       self.static_vars['Array_Output_File'], 
                                       self.static_vars['Error_File'])
                
                # We know the job is complete and so we don't want to have
                # a job dependency.
                options['previous_id'] = None
                #if not self.quiet or self.verbose:
                #    msg = "Number of array items in job: "\
                #            +str(options['arr_count'])
                #    print msg
                    
            cmd = self.__get_command(options)
            #if self.verbose:
            #    print "===================Executing==================="
            #    print "\t"+str(cmd)
            #    print "==============================================="   
            process = subprocess.Popen(cmd, shell=True,
                                        stdout=subprocess.PIPE, 
                                        stderr=subprocess.STDOUT)
            out, err = process.communicate()
            
            if process.returncode != 0:
                self.__set_fatal_error(True, "qsub failed to submit job")
            
            id = re.split('[[\.]{1}',out)[0]
            if not self.quiet or self.verbose:
                print options['name'] + " submitted. ID: "+str(id)
            if options.has_key('arr_count') and options['arr_count'] >= 0:
                if not self.quiet or self.verbose:
                    msg = "Waiting for the array job \""\
                            +str(options['name'])+"\" to finish "\
                            "before continuing ("+str(options['arr_count'])\
                            +" jobs)."
                    print msg
                
                self.__execute_job_array_check(id, 
                                               options['arr_count'], 
                                               options)
                id = None # We don't want the next job to have a dependency
                    
        else: # Standalone
            loop_count = 1
            if options.has_key('array'):
                loop_count = self.__execute_job_array_prep(
                                       options['name'], 
                                       self.static_vars['Array_Output_File'], 
                                       self.static_vars['Error_File'])
            for i in xrange(loop_count):
                options['array_id'] = i
                cmd = self.__get_command(options)
                if self.verbose:
                    print "===================Executing==================="
                    print "\t"+str(cmd)
                    print "==============================================="
                
                # Doesn't log anything.    
                #return_code = subprocess.call(cmd, shell=True)
                #if return_code != 0:
                #    msg = "The job "+str(options['name'])+" failed"
                #    self.__set_fatal_error(True, msg)
                        
                process = subprocess.Popen(cmd, shell=True,
                                          stdout=subprocess.PIPE, 
                                          stderr=subprocess.STDOUT)
                out, err = process.communicate()
                if out is not None:
                    if not self.quiet or self.verbose:
                        print str(out) # stderr is redirected to stdout
                    
                    logname = str(options['log'])
                    
                    # Mimic qsub's log naming convention
                    if options.has_key('array'):
                        logname += "-" + str(options['array_id'])
                        
                    # Write the log file
                    fp = open(logname,"w")
                    fp.write(out)
                    fp.close()
                if process.returncode != 0:
                    msg = "The job "+str(options['name'])+" failed"
                    self.__set_fatal_error(True, msg)

        return id
    def __execute_job_array_prep(self, job_name, hold_filepath, error_filepath):
        arr_count = -1
        while p.exists(hold_filepath) is not True:
            time.sleep(10)
            if p.exists(error_filepath):
                os.remove(error_filepath)
                self.__set_fatal_error(True, 
                                       err_msg="a job prior to the array "
                                       "failed and the pipeline "
                                       "cannot recover")        
        file = open(hold_filepath, "r")
        arr_count = file.readline()
        arr_count = str(arr_count).strip()
    
        os.remove(hold_filepath)
        
        #if self.verbose:
        #    print "Number of array items: "+str(int(arr_count))
    
        return int(arr_count)  
    def __execute_job_array_check(self, base_id, array_count, cmd_options):
        # This creates a list of files that should exist by the end of the
        # array job (every individual job will create either a success file
        # or a failure file).  This creates a list of those filenames and
        # then checks to see if each of them exists.  If a successful one 
        # does, it is remove from the list.  If a failure one does, the 
        # pipeline errors and exits.  Once it checks through the list, 
        # HiTSAPipe sleeps and then checks again until the entire list has
        # been deleted.  
        if self.type == "Parallel":
            complete = []

            for i in xrange(array_count):
                s, f = self.__execute_job_array_filename(cmd_options, i)
                complete.append(i)
                complete[i] = {
                    'success':    s,
                    'failure':    f,
                    'array_id':   i
                }
            while complete:
                for (id, files) in enumerate(complete):
                    if p.exists(files['success']):
                        if self.verbose:
                            msg = str(cmd_options['name'])\
                            +"["+str(files['array_id'])+"] completed."
                            print msg
                        del complete[id]
                    elif p.exists(files['failure']):
                        error_msg = str(cmd_options['name'])\
                        +"["+str(files['array_id'])+"] failed!"
                        self.__set_fatal_error(True, error_msg)
                time.sleep(10)
        return True 
    def __execute_job_array_filename(self, options, array_id=None):
        # This creates a file name in the same format as the job
        # scripts that lets us determine if a job has completed
        # successfully or not.
        # The final filename depends on whether the job was
        # submitted as an array of not.  A standalone job would
        # look like:
        #    jobname.success
        #    jobname.failure
        # An array job would look like:
        #    jobname-array_id.success
        #    jobname-array_id.failure
        
        array_filename = ""
        if array_id is not None:
            array_filename = "-"+str(array_id)
        base_filename = options['name']
        base_filename = base_filename + array_filename
        base_filename = expan(self.directories['job_status'], base_filename)
        
        success = str(base_filename)+".success"
        failure = str(base_filename)+".failure"
        
        return success, failure
                
    def __get_command(self, cmd_options):
         if self.type != "Parallel":
             # Execute in standalone.
             # By executing the command in the form of:
             # `var1=foo var2=bar /path/to/script`
             # var1 and var2 are passed to the script as variables
             # just like qsub would pass them.
             
             var_list = ""
             for (key,value) in self.preferences.iteritems():
                 var_list += str(key).upper()+"=\""+str(value)+"\" "
             for (key,value) in self.static_vars.iteritems():
                 var_list += str(key).upper()+"=\""+str(value)+"\" "
             for (key,value) in cmd_options['variables'].iteritems():
                 var_list += str(key).upper()+"=\""+str(value)+"\" "
             
             # Because these scripts are made to be called by qsub,
             # we need to fake some of the variables that get passed
             # in from qsub.  Currently the only variables used are
             # PBS_JOBNAME, PBS_O_WORKDIR and PBS_ARRAYID.
             var_list += "PBS_JOBNAME=\""
             var_list += str(cmd_options['name'])+"\" "
             
             var_list += "PBS_O_WORKDIR=\""
             var_list += str(self.directories['output'])+"\" "
             
             if cmd_options.has_key('array_id'):
                 var_list += "PBS_ARRAYID=\""
                 var_list += str(cmd_options['array_id'])+"\" "    
             
             var_list = var_list.rstrip()
             cmd = var_list + " " + str(cmd_options['script_location'])
         else:
             # Execute in parallel
             # Add all of the options that qsub needs.
             cmd = "qsub "
             cmd += "-N "+str(cmd_options['name'])+" "
             cmd += "-j oe "
             cmd += self.__get_command_qsub_mail(cmd_options)
             cmd += "-o "+str(cmd_options['log'])+" "
             cmd += "-d "+str(self.directories['output'])+" "
             cmd += "-q "+str(self.qsub_options['Queue'])+" "
             
             if cmd_options.has_key('parallel'):
                 cmd += "-l nodes="+str(self.qsub_options['Nodes'])+" "
             if cmd_options.has_key('arr_count'):
                 cmd_options['arr_count'] -= 1
                 cmd += "-t 0-"+str(cmd_options['arr_count'])+" "
                 cmd_options['arr_count'] += 1
             if cmd_options.has_key('previous_id'):
                 if cmd_options['previous_id'] is not None:
                     cmd += "-W depend=afterok:"
                     cmd += cmd_options['previous_id']+" "
             var_list = ""
             for (key,value) in self.preferences.iteritems():
                 var_list += str(key).upper()+"="+str(value)+","
             for (key,value) in self.static_vars.iteritems():
                 var_list += str(key).upper()+"="+str(value)+","
             for (key,value) in cmd_options['variables'].iteritems():
                 var_list += str(key).upper()+"="+str(value)+","
             
             var_list = var_list.rstrip(", ")
             if var_list is not None:
                 cmd += "-v "+var_list+" "
                 
             # Add the actual script to execute to the end.
             cmd += cmd_options['script_location']
         return cmd
    def __get_command_qsub_mail(self, cmd_options):
        email = ""
        notify = ""
        if self.qsub_options['Email'] is not None:
            email += "-M "+str(self.qsub_options['Email'])+" "
        if self.qsub_options['NotifyOnAbort']:
            notify += "a"
        if (self.qsub_options['NotifyOnBegin'] and
            cmd_options.has_key('first_script') and 
            cmd_options['first_script']):
            notify += "b"
        if (self.qsub_options['NotifyOnEnd'] and 
            cmd_options.has_key('final_script') and 
            cmd_options['final_script']):
            notify += "e"
        if not notify:
            notify = "n"
            
        # Create the final string.  Insert the space here as opposed to
        # in __get_command() in order to make spacing look nice if nothing
        # is returned.
        final_string = email+"-m "+notify+" "
        
        return final_string        
        

    def __configure(self, cli_options):
        # Given the location of the configuration file and the
        # options passed in from the command line, set up a
        # a dictionary with the parameters that will be used.
        # This dictionary will be used to set up all of our
        # variables that are used throughout the pipeline.
        # By using the SafeConfigParser, which is inherited from the
        # ConfigParser class, we can use string interpolation, which
        # means that we don't need command line arguments.  However, if
        # a command line argument is passed, it should overwrite everything
        # else.
         
        options = {}
        
        default_path_options = {
           'WorkDir':               expan(p.dirname(__file__)),
           "ScriptsDir":            expan(p.dirname(__file__), "scripts")
        }
        
        
        default_qsub_options = {
            'Execution':            'Standalone',
            'Email':                None,
            'NotifyOnAbort':        'True',
            'NotifyOnBegin':        'True',
            'NotifyOnEnd':          'True',
            'Queue':                'tiny',
            'Nodes':                '10'
        }
        
        default_sequencing_options = {
            'Database':             '/mnt/home/cblair/rdp/species/species',
            'InputSequences':       '%(WorkDir)s/input/sequences',
            'ReferenceStrains':     '%(WorkDir)s/input/RefStrains',
            'BlastSequences':       '%(WorkDir)s/input/af243169.for',
            'Suffix':               '.fasta',
            'Direction':            'Forward',
            'CutoffLength':         '50',
            'MaxBlasts':            '50',
            'MinSequenceLength':    '300',
            'NHits':                '25',
            'NPercent':             '.01',
            'Root':                 'Methanococcus_jannaschii',
            'Primer3':              'GACTCGGTCC',
            'Primer5':              'CCTAGTGGAGG'
        }
        
        # Default options
        defaults = dict(default_path_options.items() +
                       default_qsub_options.items() + 
                       default_sequencing_options.items())
        
        c = ConfigParser.SafeConfigParser(defaults)
        c.read(cli_options.config_file)
        
        if c.has_section("Pipeline") is not True:
            c.add_section("Pipeline")
        if c.has_section("qsub") is not True:
            c.add_section("qsub")
        
        if cli_options.work_dir is not None:
            c.set("Pipeline", "WorkDir", cli_options.work_dir)
        if cli_options.scripts_dir is not None:
            c.set("Pipeline", "ScriptsDir", cli_options.scripts_dir)
        
        final_options = {
             'WorkDir':             expan(c.get("Pipeline", "WorkDir")),
             'ScriptsDir':          expan(c.get("Pipeline", "ScriptsDir")),
             'Debug':               cli_options.debug,
             'Verbose':             cli_options.verbose,
             'Quiet':               cli_options.quiet,

             'Execution':           c.get("qsub", "Execution"),        
             'Email':               c.get("qsub", "Email"),
             'NotifyOnAbort':       c.getboolean("qsub", "NotifyOnAbort"),
             'NotifyOnBegin':       c.getboolean("qsub", "NotifyOnBegin"),
             'NotifyOnEnd':         c.getboolean("qsub", "NotifyOnEnd"),
             'Queue':               c.get("qsub", "Queue"),
             'Nodes':               c.get("qsub", "Nodes"),
        
             'Database':            expan(c.get("Pipeline", "Database")),
             'InputSequences':      expan(c.get("Pipeline", "InputSequences")),
             'ReferenceStrains':    expan(c.get("Pipeline",
                                                "ReferenceStrains")),
             'BlastSequences':      expan(c.get("Pipeline", "BlastSequences")),
             'Suffix':              c.get("Pipeline", "Suffix"),
             'Direction':           c.get("Pipeline", "Direction"),
             'CutoffLength':        c.get("Pipeline", "CutoffLength"),
             'MaxBlasts':           c.get("Pipeline", "MaxBlasts"),
             'MinSequenceLength':   c.get("Pipeline", "MinSequenceLength"),
             'NHits':               c.get("Pipeline", "NHits"),
             'NPercent':            c.get("Pipeline", "NPercent"),
             'Root':                c.get("Pipeline", "Root"),
             'Primer3':             c.get("Pipeline", "Primer3"),
             'Primer5':             c.get("Pipeline", "Primer5"),
        }
        
        # As clean as this part is, it doesn't take defaults into account.
        # This is why we use the explicit calls above instead.
        #for sections in parser.sections():
        #    for (key,value) in c.items(sections):
        #        final_options[key] = c.get(sections, key) 
        
        
        # Check to make sure that the "Shared" environment variable
        # is set.  If it isn't, check to see if there is a .ncbirc file
        # in the user's home directory.  If either of these conditions
        # are met, set MPIBlastDB to the directory in that variable.
        # __validate() will check to see if this is set and if not, force
        # standalone mode because mpiBLAST will fail otherwise.
          
        mpiblastdb = None
        if final_options['Execution'] == "Parallel":    
            mpiblastdb = os.getenv("Shared", default=None)
            if mpiblastdb is None:
                ncbi_parser = ConfigParser.SafeConfigParser()
                ncbi_parser.read(expan("~/.ncbirc"))
                if ncbi_parser.has_section("mpiBLAST"):
                    if ncbi_parser.has_option("mpiBLAST", "Shared"):
                        mpiblastdb = ncbi_parser.get("mpiBLAST",
                                                    "Shared")
        if mpiblastdb is not None:
            mpiblastdb = expan(mpiblastdb)
        final_options['MPIBlastDB'] = mpiblastdb
        
        # Finally, run through the options and make sure anything that
        # anything that is blank is set to None.  This is because the
        # ConfigParser converts everything to strings.
        for (key,value) in final_options.items():
            #if str(value).lower() == "none" or not value:
            if not value:
                final_options[key] = None
        
        return final_options
    def __validate(self):
        # Because the configuration parser has default values built in, the
        # only validation that's currently necessary is that the scripts
        # directory and work directory exist.
        # This should check to make sure the .ncbirc file exists and that
        # the local and shared directories under mpiBLAST exist.

        is_valid = p.exists(self.params['ScriptsDir'])
        self.__set_fatal_error(is_fatal=not is_valid,
                               err_msg="The scripts directory doesn't exist.")
        
        is_valid = p.exists(self.params['WorkDir'])
        self.__set_fatal_error(is_fatal=not is_valid,
                               err_msg="The working directory doesn't exist.")
        
        if self.params['Execution'] == "Parallel":
            if self.params['MPIBlastDB'] is None:
                #if not self.quiet or self.verbose:
                print "WARNING: mpiBLAST needs variable 'Shared' in order "\
                "to execute.  Check that this variable is set (usually in "\
                "~/.ncbirc).  Will run in Standalone mode."
                            
                                 
    def __get_directories(self):
        # Return a dictionary of the directories, based on the parameters
        # that were configured.
        
        d = {
             'work':                    self.params['WorkDir'],
             'scripts':                 self.params['ScriptsDir']
        }
        d['output']         =           expan(d['work'],'output')
        d['logs']           =           expan(d['output'],'logs')
        d['job_status']     =           expan(d['output'], 'tmp')
        d['references']     =           expan(d['output'],'references')
        d['blast']          =           expan(d['output'],'blast')
        d['clustal']        =           expan(d['output'],'clustal')
        d['final']           =          expan(d['output'],'final')
        
        d['originals']      =           expan(d['references'],
                                                     'originals')
        
        d['blast_input']    =           expan(d['blast'],'input')
        d['blast_temp']     =           expan(d['blast'],'tmp')
        d['blast_output']   =           expan(d['blast'],'blasts')
        
        d['clustal_temp']   =           expan(d['clustal'],'tmp')
        
        # If not in parallel, this will be None
        d['blast_db']       =           self.params['MPIBlastDB'] 
        
        return d
    def __get_preferences(self):
        # Return a dictionary of the preferences variables, based on the
        # parameters that were configured.
        # If qsub isn't found on the system and the execution was
        # set to be parallel, then execution is set
        # to be forced into standalone.
        # The 'Debug' preference currently is a command line argument and 
        # if the debug flag is true then both this pipeline class and the 
        # scripts it executes be set by the same flag.  The script's debug
        # option could instead come from the configuration file.
         
        c = {
             'Execution':               self.params['Execution'],
             'Database':                self.params['Database'],
             'Input_Sequences':         self.params['InputSequences'],
             'Reference_Strains':       self.params['ReferenceStrains'],
             'Blast_Sequences':         self.params['BlastSequences'],
             'Suffix':                  self.params['Suffix'],
             'Direction':               self.params['Direction'],
             'Cutoff_Length':           self.params['CutoffLength'],
             'Max_Blasts':              self.params['MaxBlasts'],
             'Min_Sequence_Length':     self.params['MinSequenceLength'],
             'NHits':                   self.params['NHits'],
             'NPercent':                self.params['NPercent'],
             'Root':                    self.params['Root'],
             'Primer3':                 self.params['Primer3'],
             'Primer5':                 self.params['Primer5'],
             'Debug':                   self.params['Debug']                          
        }
        # Make sure qsub exists on the system
        if c['Execution'] == 'Parallel':
            does_qsub_exist = subprocess.Popen(["which","qsub"],
                                               stdout=subprocess.PIPE,
                                               stdin=subprocess.PIPE,
                                               stderr=subprocess.PIPE,
                                               shell=False)
            does_qsub_exist.communicate() # Don't care about output.
            if does_qsub_exist.returncode != 0:
                c['Execution'] = "Forced Standalone - qsub not found"
            if self.params['MPIBlastDB'] is None:
                c['Execution'] = "Forced Standalone - mpiBLAST "\
                                 "configuration error"
                
        return c
    def __get_static_vars(self):
        # Returns a dictionary of variables that are used between the scripts
        # to make sure that anything that uses the same variable is
        # always pointing to the same value.
        
        s = {
            'Perl_Dir':                 self.directories['scripts'],
            'Scripts_Dir':              self.directories['scripts'],
            'References_Dir':           self.directories['references'],
            'Originals_Dir':            self.directories['originals'],
            'Log_Dir':                  self.directories['logs'],
            'Blast_Dir':                self.directories['blast'],
            'Blast_Temp_Dir':           self.directories['blast_temp'],
            'Blastall_Output_Dir':      self.directories['blast_output'],
            'MPIBlast_Shared':          self.directories['blast_db'],
            'Hit_Output_Dir':           self.directories['clustal'],
            'Clustal_Output_Dir':       self.directories['clustal'],
            'Clustal_Temp_Dir':         self.directories['clustal_temp'],
            'Neighbor_Dir':             self.directories['clustal'],
            'Final_Dir':                self.directories['final'],
            'NNodes':                   self.params['Nodes']
        }
        s['Good_Sequences_File'] = expan(self.directories['blast_input'],
                                                "good_sequences")
        s['Numseqs_Temp_File'] = expan(s['Blast_Temp_Dir'],
                                              "numseqs.tmp")
        s['Blast_Input_File'] = expan(self.directories['blast_input'],
                                             "blast_input")
        s['Hit_File'] = expan(s['Clustal_Output_Dir'], "hitfiles")
        s['Hit_Names_File'] = expan(s['Clustal_Output_Dir'], "hitnames")
        s['Output_XLS_One'] = expan(s['Clustal_Output_Dir'], "output1.xls")
        s['Output_XLS_Five'] = expan(s['Clustal_Output_Dir'], "output5.xls")
        s['Clustal_All_File'] = expan(s['Clustal_Output_Dir'],
                                             "clustalall")
        s['Clustal_Alignment_File'] = expan(s['Clustal_Output_Dir'],
                                                   "clustalall.aln")
        s['Clustal_Phylip_File'] = expan(s['Clustal_Output_Dir'],
                                                   "clustalall.phy")                                                   
        s['Phylip_In_File'] = expan(s['Clustal_Output_Dir'],
                                            "infile")
        s['Alignment_Points_File'] = expan(s['Clustal_Output_Dir'],
                                            "alignmentpoints")
        s['Job_Status_Dir'] = expan(self.directories['job_status'])
        s['Helper_Functions'] = expan(s['Scripts_Dir'], "helpers")
        s['Array_Output_File'] = expan(self.directories['output'],
                                              "arrayjob.tmp")
        s['Error_File'] = expan(self.directories['output'], "error.tmp")
        return s
    def __get_qsub_options(self):
        # Return a dictionary of the options to be passed to qsub, based
        # on the parameters that were configured.
        
        q = {
             'Email':                   self.params['Email'],
             'NotifyOnAbort':           self.params['NotifyOnAbort'],
             'NotifyOnBegin':           self.params['NotifyOnBegin'],
             'NotifyOnEnd':             self.params['NotifyOnEnd'],
             'Queue':                   self.params['Queue'],
             'Nodes':                   self.params['Nodes']
        }
        return q
    def __setup_directory_structure(self):
        # This method removes the 'output' directory if it exists
        # and then recreates the directory and any subdirectories
        # that it needs.  Because the directories dictionary isn't
        # an ordered dictionary (not avail in Python 2.6.6), we can't
        # loop through the dictionary easily to create the subdirectories.
        
        # Remove the output directory if it exists and recreate it
        if p.exists(self.directories['output']):
            shutil.rmtree(self.directories['output'])
        os.mkdir(self.directories['output'])
        
        # Subdirectories of 'output'
        os.mkdir(self.directories['logs'])
        os.mkdir(self.directories['job_status'])
        os.mkdir(self.directories['references'])
        os.mkdir(self.directories['blast'])
        os.mkdir(self.directories['clustal'])
        os.mkdir(self.directories['final'])
        
        # Subdirectories of 'blast'
        os.mkdir(self.directories['blast_input'])
        os.mkdir(self.directories['blast_temp'])
        os.mkdir(self.directories['blast_output'])
        
        # Subdirectories of 'clustal'
        os.mkdir(self.directories['clustal_temp'])
        
        # Make sure that the folder specified in the .ncbirc
        # exists and create it otherwise.
        if self.type == "Parallel":
            if not os.path.exists(self.directories['blast_db']):
                os.mkdir(self.directories['blast_db'])
    def __backup_configuration(self):
        backup_list = {
           'qsub Options':              self.qsub_options,
           'Pipeline Preferences':      self.preferences
       }
        parser = ConfigParser.ConfigParser()
        parser.optionxform = str
        
        for (section,var) in backup_list.iteritems():
            parser.add_section(section)
            for (option, value) in var.iteritems():
                parser.set(section, option, value)
        
        filename = expan(self.directories['references'],
                                'used_preferences.conf')
        fp = open(filename,"w")
        parser.write(fp)
        fp.close()
    def __check_for_fatal(self):
        return self.__fatal_error
    def __set_fatal_error(self, is_fatal, err_msg=""):
        if not self.__check_for_fatal():
            self.__fatal_error = is_fatal
        if self.__check_for_fatal():
            print >> sys.stderr, "HiTSAPipe ERROR: "+err_msg+" Exiting."
            sys.exit(1)

            
def main():
    # Set up an OptionParser object to make the command line arguments easy.
    # NOTE:  optparse has been deprecated since Python 2.7, but as of May 2013,
    #        the cluster is using Python 2.4.3. argparse should be used once
    #        a newer version of Python is in use.
    
    parser = OptionParser(usage="%prog [options] \n"
                          "HiTSA Pipeline for Beowulf cluster using TORQUE\n"
                          "Results may differ between standalone and parallel "
                          "executions due to different versions of blastall "
                          "and clustalw being used.",
                          version="%prog (HiTSAPipe) v0.4.0")
    parser.add_option("-c", "--config", action="store", dest="config_file",
                      help="the file where the your preferences are "
                            "located [default: %default]")
    
    adv_group = OptionGroup(parser, "Advanced Options", 
                            "Use these options at your own risk. "
                            "Changing these options could cause "
                            "the program to fail.")
    adv_group.add_option("-w", "--work", action="store", dest="work_dir",
                         help="the working directory where all output will be "
                         "placed - this will overwrite the configuration "
                         "file [default: %default]")
    adv_group.add_option("-s", "--scripts", action="store", dest="scripts_dir",
                         help="the directory where the scripts are located - "
                         "this will overwrite the configuration file "
                         "[default: %default]")
    parser.add_option_group(adv_group)
    
    
    output_group = OptionGroup(parser, "Output Options",
                               "Use these to change what output is displayed "
                               "in from HiTSAPipe or from the individual jobs")
    output_group.add_option("-v", "--verbose",
                            action="store_true",
                            dest="verbose",
                            help="show additional, more detailed output "
                            "that HiTSAPipe generates - overrides quiet mode "
                            "[default: %default]")
    output_group.add_option("-q", "--quiet",
                            action="store_true",
                            dest="quiet",
                            help="hides all output, except for errors, "
                            "that HiTSAPipe generates [default: %default]")
    output_group.add_option("-d", "--debug",
                            action="store_true",
                            dest="debug",
                            help="show additional, more detailed output "
                            "that the jobs generate [default: %default]")
    parser.add_option_group(output_group)
    
    # Set up the defaults
    parser.set_default("verbose", False)
    parser.set_default("quiet", False)
    parser.set_default("debug", False)
    parser.set_default("config_file", expan(p.dirname(__file__),
                                            "./preferences.conf"))
    
    # Parse the arguments and return a tuple of the options and the
    # any leftover arguments.  If there are any leftover arguments, we
    # will throw an error.
    (options, remaining_arguments) = parser.parse_args()
    
    if len(remaining_arguments) > 0:
        # An incorrect number of arguments was given.
        # Pass an error message to the parser's error function that will
        # display the error, proper usage and then exit.
        error_message = "There were an incorrect number of arguments passed.\n"
        error_message += "The argument(s) that couldn't be parsed were:\n"
        for arg in remaining_arguments:
            error_message += arg+"\n"
        error_message += "Please see the usage above and try again."
        parser.error(error_message)
    
    # Handle user input   
    options.scripts_dir = expan(options.scripts_dir)
    options.work_dir = expan(options.work_dir)   
    options.config_file = expan(options.config_file)
    
    # Create a Pipeline object, run it and exit.
    pipeline = Pipeline(options)
    sys.exit(pipeline.run())

    
if __name__ == '__main__':
    main()